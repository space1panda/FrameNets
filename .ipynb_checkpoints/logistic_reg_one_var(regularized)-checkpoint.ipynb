{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-926a54cdf9b6>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-926a54cdf9b6>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    m = y(:,1) # num of training exanples\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import csv\n",
    "import matplotlib\n",
    "#make imports....\n",
    "\n",
    "\n",
    "alpha = 0.3\n",
    "reg_lambda = 0.01\n",
    "num_iterations = 1000\n",
    "#theta = np.zeros(n,n)\n",
    "\n",
    "\n",
    "\n",
    "with open(\".csv\") as file:\n",
    "    #export training set for classification to the following variables:\n",
    "    #X - training matrix (np.asarray());\n",
    "    #y - vertical vector with correct outputs;\n",
    "\n",
    "m = y(:,1) # num of training exanples    \n",
    "    \n",
    "def sigmoid(z):\n",
    "    g = 1 /(1+ np.exp(-z))\n",
    "    return gpl\n",
    "\n",
    "#use standard sigmoid function Octave: g = 1./(1+exp(-z)) where z is X * theta\n",
    "\n",
    "    \n",
    "J = #use standard regulzarized cost function with sigmoid and lambda regularization\n",
    "\n",
    "\n",
    "gradient = #use standard regularized gradient formula (octave ex.)\n",
    "\n",
    "\n",
    "\n",
    "i = 1:num_iterations\n",
    "\n",
    "for i<num_iterations:\n",
    "    theta = theta - alpha*gradient\n",
    "    print(theta)\n",
    "    print(J(theta))\n",
    "\n",
    "\n",
    "#Show J-minimization on graph\n",
    "    \n",
    "    \n",
    "#Notes from Octave:\n",
    "\n",
    "    \n",
    "#J = (1/m)*((-y')*log(sigmoid(X*theta))-((1-y)')*log(1-sigmoid(X*theta)))+(lambda/(2*m))*sum(theta(2:end).^2);\n",
    "\n",
    "#grad = (1/m)*((X'*(sigmoid(X*theta)-y))) +(lambda/m)*theta;\n",
    "\n",
    "#grad(1) = grad(1)-(lambda/m)*theta(1) (don't regularize theta(0));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Check accuracy on the training set (sigmoid >=0.5)\n",
    "\n",
    "predictions = np.zeros(m,1)\n",
    "\n",
    "Octave:\n",
    "\n",
    "\n",
    "#for i=1:m,\n",
    " # if sigmoid(theta'*X(i,:)') >=0.5,\n",
    "  #  p(i,:) = 1;\n",
    "  #endif;\n",
    "  #endfor;\n",
    "\n",
    "#Count # of m where prediction = y / m *100% = accuracy ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions \n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "reg_lambda = 0.01\n",
    "num_iterations = 2000\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    g = 1 /(1+ np.exp(-z))\n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "def RegCostFunction():\n",
    "    J = -1/m * np.sum(y * np.log(sigmoid(np.dot(X,theta))) + (1-y) * (np.log(1-sigmoid(np.dot(X,theta)))))\n",
    "    return J\n",
    "\n",
    "def CalcGradient():\n",
    "    grad = (1/m)*np.dot(X.transpose(),(sigmoid(np.dot(X,theta))-y))\n",
    "    return grad\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is 0.6931471805599452\n",
      "Minimized Cost is 0.4735814139029435\n",
      "Minimized paremeters are [ 1.50825163 -0.21818018 -0.12990941]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "226.82549113596622"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv('diabet.csv')\n",
    "\n",
    "training = np.array(data)\n",
    "\n",
    "X = np.array(training[0:400,:2], dtype = np.float32)\n",
    "[m,n] = X.shape\n",
    "X = np.concatenate((np.ones((m,1)), X), axis=1)\n",
    "y = np.array(training[0:400,3], dtype = np.float32) #86 left for testing\n",
    "theta = np.zeros((n+1,), dtype = np.float32)\n",
    "\n",
    "grad = (1/m)*np.dot(X.transpose(),(sigmoid(np.dot(X,theta))-y))\n",
    "\n",
    "print(\"Initial Cost is\",RegCostFunction())\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    theta = theta - alpha*CalcGradient()\n",
    "\n",
    "\n",
    "print(\"Minimized Cost is\", RegCostFunction())\n",
    "\n",
    "print(\"Minimized paremeters are\", theta)\n",
    "\n",
    "predictions = np.zeros((m,1))\n",
    "\n",
    "#for i in range(m):\n",
    "   # if sigmoid(np.dot(X[i,:], theta)) >= 0.5:\n",
    "       # predictions[i] = 1\n",
    "\n",
    "test = [196,198,197]\n",
    "\n",
    "\n",
    "theta.shape\n",
    "test = np.array(test)\n",
    "test.shape\n",
    "np.dot(test,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
