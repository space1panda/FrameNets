{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')\n",
    "\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "\n",
    "def initialize_parameters(hidden_size, vocab_size):\n",
    "    \n",
    "    Wax = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "    b = np.zeros((hidden_size, 1)) # hidden bias\n",
    "    by = np.zeros((vocab_size, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def update_parameters_lronly(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] -= lr * gradients['dWax']\n",
    "    parameters['Waa'] -= lr * gradients['dWaa']\n",
    "    parameters['Wya'] -= lr * gradients['dWya']\n",
    "    parameters['b']  -= lr * gradients['db']\n",
    "    parameters['by']  -= lr * gradients['dby']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    indices = []\n",
    "    idx = -1 \n",
    "    \n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while idx != newline_character:\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = softmax(z)\n",
    "        p = y\n",
    "        idx = np.random.choice(list(range(y.size)), p = p.ravel())\n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n): yield l[i:i + n]\n",
    "\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v, ord=1)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource\n",
    "\n",
    "#### returns input-output object to be taken to batcher  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDatasource:\n",
    "    \n",
    "    def __init__(self, path, seq_len, *args):\n",
    "        \n",
    "        \"\"\"Initialize vocabularies for char rnn (w||w/o lstm). \n",
    "        In our case we want to train only characters and < (end of word) token\n",
    "         Cleaning data with regex: \\n character left to indicate end of token(word) \n",
    "        to train  ending word, cleaning all symbols and spaces\n",
    "        \"\"\"\n",
    "        \n",
    "        data = open(path, 'r').read().lower()\n",
    "        data = re.sub('[^0-9a-zA-Z\\n]+', '', data)\n",
    "        chars = list(set(data))\n",
    "        \n",
    "        self._seq_len = seq_len\n",
    "        self._char2ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "        self._ix2char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "        self._vocab_size = len(self._char2ix)\n",
    "        \n",
    "        \"\"\"Initialize vocabulary with char appearance statistics. \n",
    "        It can be useful to determine unfrequently used characters to omit them in training. \n",
    "        In this experiment however we are not excluding characters due to small dataset.  \n",
    "        \"\"\"\n",
    "        \n",
    "        self._char_stats = {char:0 for char in sorted(chars)}\n",
    "        \n",
    "        # Initialize x, y sets as empty lists\n",
    "        \n",
    "        self._tokens = []\n",
    "        self._targets = []\n",
    "        \n",
    "        \"\"\"generator function for getting dataset  desired sequence length. We are not setting \n",
    "        sequence length to the length of token to apply vectorization without padding.\n",
    "        This is a utility function which is not supposed to be inside of the object\n",
    "        \"\"\"\n",
    "        \n",
    "        def chunks(l, n):\n",
    "            for i in range(0, len(l), n): yield l[i:i + n]\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"Call generator to encode input and output sequences. \n",
    "        Output is a one-step forward sequence in the same datasource - \n",
    "        We want our model to learn predicting next character\n",
    "        \"\"\"\n",
    "        \n",
    "        source = chunks(data, seq_len)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                s = next(source)\n",
    "                for ch in s:\n",
    "                    self._char_stats[ch]+=1\n",
    "                self._tokens.append([self._char2ix['\\n']]+[self._char2ix[ch] for ch in s])\n",
    "                self._targets.append([self._char2ix[ch] for ch in s]+[self._char2ix['\\n']])\n",
    "        \n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        \"\"\"Vectorize with numpy and quickly one-hot the input set.\n",
    "        Final shapes = tokens - (num_of_sequences, sequence length, vocab_size)\n",
    "        targets - (num_of_sequences, sequence length, char2idx[character])\n",
    "        We are not creating one-hot for outputs because we want to count loss \n",
    "        between only most-probable character index and target\n",
    "        \"\"\"\n",
    "        \n",
    "        self._tokens = np.array(self._tokens[:-1], dtype=np.int32)\n",
    "        self._targets = np.array(self._targets[:-1])\n",
    "        x_values = np.max(self._tokens)+1\n",
    "        self._tokens = np.eye(x_values)[self._tokens]\n",
    "    \n",
    "    # TO-DO: @properties\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._tokens[idx], self._targets[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "#### Neural net architecture for raw RNN - to be called once per epoch. Returns gradients and last hidden state from sequence to iterate through batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModelRNN:\n",
    "    \n",
    "    def __init__(self, clip_ratio, params):\n",
    "        \n",
    "        \"\"\" Initialize necessary hyperparams and nn layers. Import updated params \n",
    "        (or zero params).\n",
    "        Reset all grads \n",
    "        \"\"\"\n",
    "        \n",
    "        self._clip_ratio = clip_ratio\n",
    "        self._Wax = params['Wax']\n",
    "        self._Waa = params['Waa']\n",
    "        self._Wya = params['Wya']\n",
    "        self._b = params['b']\n",
    "        self._by = params['by']\n",
    "        \n",
    "        \n",
    "    def forward_rnn(self, X, Y, hidden_back):\n",
    "        \n",
    "        hidden, output = {}, {}\n",
    "        \n",
    "        hidden[-1]= hidden_back\n",
    "        loss = 0\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            hidden[t] = np.tanh(np.dot(self._Wax, X[:,t].T)+ \n",
    "                                np.dot(self._Waa, hidden[t-1])+ self._b)\n",
    "            preoutput = np.dot(self._Wya, hidden[t]) + self._by\n",
    "            output[t] = np.exp(preoutput) / np.sum(np.exp(preoutput))\n",
    "            \n",
    "            loss-= np.sum(np.log(output[t].T[range(len(X)),Y[:,t]])) / len(X)\n",
    "            \n",
    "        cache = (output, hidden)   \n",
    "       \n",
    "        return loss, cache\n",
    "    \n",
    "    def rnn_backward(self, X, Y, cache):\n",
    "    \n",
    "        gradients = {}\n",
    "        output, hidden = cache\n",
    "        gradients['dWax'] = np.zeros_like(self._Wax) \n",
    "        gradients['dWaa'] = np.zeros_like(self._Waa)\n",
    "        gradients['dWya'] = np.zeros_like(self._Wya)\n",
    "        gradients['db'] = np.zeros_like(self._b)\n",
    "        gradients['dby'] = np.zeros_like(self._by)\n",
    "        gradients['da_next'] = np.zeros_like(hidden[0])\n",
    "\n",
    "        for t in reversed(range(seq_len)):\n",
    "            dy = np.copy(output[t])\n",
    "            dy[Y[:,t]] -= 1\n",
    "            gradients['dWya'] += np.dot(dy, hidden[t].T)\n",
    "            gradients['dby'] += np.sum(dy, axis=1).reshape(-1,1)\n",
    "            da = np.dot(self._Wya.T, dy) + gradients['da_next']\n",
    "\n",
    "            daraw = (1 - hidden[t] * hidden[t] ) * da\n",
    "            gradients['db'] += np.sum(daraw, axis=1).reshape(-1,1)\n",
    "            gradients['dWax'] += np.dot(daraw, X[:,t])#.reshape(1,-1))\n",
    "            gradients['dWaa'] += np.dot(daraw, hidden[t-1].T)\n",
    "            gradients['da_next'] = np.dot(self._Waa.T, daraw)\n",
    "\n",
    "        return gradients, hidden[t-1]\n",
    "        \n",
    "    def optimize(self, X, Y, hidden_back):\n",
    "\n",
    "        loss, cache = self.forward_rnn(X, Y, hidden_back)\n",
    "        gradients, hidden_back = self.rnn_backward(X, Y, cache)\n",
    "        gradients = clip(gradients, self._clip_ratio)\n",
    "        return loss, gradients, hidden_back\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader/Batcher\n",
    "#### Generate input/output batches from datasource for train/test modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataLoader(CharDatasource):\n",
    "    \n",
    "    def __init__(self, batch_size, **kwargs):\n",
    "        \n",
    "        \"\"\" CharDatasource is used as parent object.\n",
    "        train/test mode defines which part of dataset to use for batching\n",
    "        \"\"\"\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \"\"\"Just to indicate - we are encountering chunks utility function for the second time.\n",
    "        In project this should go to utility - otherwise it's a bad practice\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"Splitting dataset into train/test\n",
    "        \"\"\"\n",
    "        \n",
    "        train_set = (self._tokens, self._targets)\n",
    "        \n",
    "        \"\"\"Eventhough we are having pretty small dataset, it's a \n",
    "        good practice to use generators instead of keeping\n",
    "        additional data arrays to free up memory\n",
    "        \"\"\"\n",
    "        \n",
    "        x_loader = chunks(train_set[0], self._batch_size)\n",
    "        y_loader = chunks(train_set[1], self._batch_size)\n",
    "\n",
    "            \n",
    "        self._batches=[]\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                x, y = next(x_loader), next(y_loader)\n",
    "                if len(x)==self._batch_size:\n",
    "                    self._batches.append((x,y))\n",
    "\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def _getbatch(self, idx):\n",
    "        return self._batches[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "#### Trainer brings together everything we created before, to iterate via epochs and update parameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTrainer:\n",
    "    \n",
    "    def __init__(self, lr, n_epochs, hidden, vocab, batch_size, seq_len,\n",
    "                 clip_ratio, path, **kwargs):\n",
    "        self._lr = lr\n",
    "        self._n_epochs = n_epochs\n",
    "        self._hidden = hidden\n",
    "        self._vocab = vocab\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self._parameters = initialize_parameters(self._hidden, self._vocab)\n",
    "\n",
    "        self._dataload = RNNDataLoader(batch_size=batch_size, \n",
    "                           path=path,  seq_len=seq_len)\n",
    "\n",
    "\n",
    "        self._model = CharModelRNN(clip_ratio, self._parameters)\n",
    "\n",
    "    def train(self):\n",
    "        train_epoch_loss = {}\n",
    "        \n",
    "        batches = self._dataload._batches\n",
    "        loss = get_initial_loss(self._vocab, self._batch_size)\n",
    "        hidden_back = np.zeros((self._hidden, self._batch_size))\n",
    "        \n",
    "        for j in range(self._n_epochs):\n",
    "        \n",
    "            for index in range(len(batches)):\n",
    "                \n",
    "                X = batches[index][0]\n",
    "                Y = batches[index][1]\n",
    "\n",
    "                curr_loss, gradients, hidden_back = self._model.optimize(X, Y, \n",
    "                                                                         hidden_back)\n",
    "\n",
    "                curr_loss = np.mean(curr_loss)\n",
    "                loss = smooth(loss, curr_loss)\n",
    "                train_epoch_loss.update({j:loss})\n",
    "                \n",
    "                self._parameters = update_parameters_lronly(self._parameters, \n",
    "                                                     gradients, self._lr)\n",
    "\n",
    "            if j % (self._n_epochs/10) == 0:\n",
    "\n",
    "                print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "\n",
    "                seed = 0\n",
    "                \"\"\"\n",
    "                for name in range(5):\n",
    "                    \n",
    "                    sampled_indices = sample(self._parameters, \n",
    "                                             self._dataload._char2ix, seed)\n",
    "                    print_sample(sampled_indices, self._dataload._ix2char)\n",
    "                \"\"\" \n",
    "        plt.plot(train_epoch_loss.values())\n",
    "        f = open(\"params_char_lmodel_fairycountry.pkl\",\"wb\")\n",
    "        pickle.dump(self._parameters,f)\n",
    "        f.close()\n",
    "        return self._parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "#### Probing data with custom hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "hidden = 10\n",
    "vocab = 27\n",
    "batch_size = 1\n",
    "seq_len = 15\n",
    "path = 'test.txt'\n",
    "clip_ratio = 5\n",
    "lr = 0.003\n",
    "\n",
    "trainer = CharTrainer(lr=lr, clip_ratio=clip_ratio,\n",
    "                     path=path, seq_len=seq_len,\n",
    "                     batch_size=batch_size, \n",
    "                     vocab = vocab, \n",
    "                     hidden=hidden, n_epochs = n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 10.388622\n",
      "\n",
      "Iteration: 50, Loss: 37.806180\n",
      "\n",
      "Iteration: 100, Loss: 35.572734\n",
      "\n",
      "Iteration: 150, Loss: 34.674363\n",
      "\n",
      "Iteration: 200, Loss: 34.209972\n",
      "\n",
      "Iteration: 250, Loss: 33.940058\n",
      "\n",
      "Iteration: 300, Loss: 33.876025\n",
      "\n",
      "Iteration: 350, Loss: 33.633727\n",
      "\n",
      "Iteration: 400, Loss: 33.511995\n",
      "\n",
      "Iteration: 450, Loss: 33.407437\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "### Testing model on new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ania\n",
      "Egesovia\n",
      "\n",
      "Ia\n",
      "Othamorize\n",
      "\n",
      "An\n",
      "Iava\n",
      "Alerdolai\n",
      "\n",
      "Ha\n",
      "E\n",
      "Hbecoat\n",
      "Iy\n",
      "Icloussa\n",
      "Itistan\n",
      "Ia\n",
      "Omonia\n",
      "An\n",
      "Ena\n"
     ]
    }
   ],
   "source": [
    "datas = CharDatasource('test.txt', 15)\n",
    "\n",
    "with open('params_char_lmodel_fairycountry.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "for i in range(20):\n",
    "    sampled_indices = sample(params, datas._char2ix,0)\n",
    "    print_sample(sampled_indices, datas._ix2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
