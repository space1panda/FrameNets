{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from string import ascii_lowercase\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from random import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource\n",
    "\n",
    "#### returns get method for one input-output sample to be taken to batcher  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDatasource:\n",
    "    \n",
    "    def __init__(self, path, seq_len, *args):\n",
    "        \n",
    "        \"\"\"Initialize vocabularies for char rnn (w||w/o lstm). \n",
    "        In our case we want to train only characters and < (end of word) token\"\"\"\n",
    "        self._seq_len = seq_len\n",
    "        self._char2idx = {'<':0}\n",
    "        self._char2idx.update({char:idx+1 for (idx, char) \n",
    "                               in enumerate(ascii_lowercase)})\n",
    "        self._idx2char = ['<']\n",
    "        self._idx2char.extend([char for char in ascii_lowercase])\n",
    "        self._vocab_size = len(self._char2idx)\n",
    "        \n",
    "        \"\"\"Initialize vocabulary with char appearance statistics. \n",
    "        It can be useful to determine unfrequently used characters to omit them in training. \n",
    "        In this experiment however we are not excluding characters due to small dataset.  \n",
    "        \"\"\"\n",
    "        \n",
    "        self._char_stats = {'<':0}\n",
    "        self._char_stats.update({char:0 for char in ascii_lowercase})\n",
    "        \n",
    "        # Initialize x, y sets as empty lists\n",
    "        \n",
    "        self._tokens = []\n",
    "        self._targets = []\n",
    "        \n",
    "        \"\"\"generator function for getting dataset  desired sequence length. We are not setting \n",
    "        sequence length to the length of token to apply vectorization without padding.\n",
    "        This is a utility function which is not supposed to be inside of the object\n",
    "        \"\"\"\n",
    "        \n",
    "        def chunks(l, n):\n",
    "            for i in range(0, len(l), n): yield l[i:i + n]\n",
    "                \n",
    "        \"\"\" Cleaning data with regex: < character left to indicate end of token(word) \n",
    "        to train  ending word, cleaning all symbols and spaces\n",
    "        \"\"\"\n",
    "        \n",
    "        data = open(path, 'r').read().lower().replace('\\n','<')\n",
    "        data = re.sub('[^0-9a-zA-Z<]+', '', data)\n",
    "        \n",
    "        \"\"\"Call generator to encode input and output sequences. \n",
    "        Output is a one-step forward sequence in the same datasource - \n",
    "        We want our model to learn predicting next character\n",
    "        \"\"\"\n",
    "        \n",
    "        source = chunks(data, seq_len)\n",
    "        target = chunks(data, seq_len+1)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                s = next(source)\n",
    "                t = next(target)[1:]\n",
    "                for ch in s:\n",
    "                    self._char_stats[ch]+=1\n",
    "                self._tokens.append([self._char2idx[ch] for ch in s])\n",
    "                self._targets.append([self._char2idx[ch] for ch in t])\n",
    "        \n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        \"\"\"Vectorize with numpy and quickly one-hot the input set.\n",
    "        Final shapes = tokens - (num_of_sequences, sequence length, vocab_size)\n",
    "        targets - (num_of_sequences, sequence length, char2idx[character])\n",
    "        We are not creating one-hot for outputs because we want to count loss \n",
    "        between only most-probable character index and target\n",
    "        \"\"\"\n",
    "        \n",
    "        self._tokens = np.array(self._tokens[:-1], dtype=np.int32)\n",
    "        self._targets = np.array(self._targets[:-1])\n",
    "        x_values = np.max(self._tokens)+1\n",
    "        self._tokens = np.eye(x_values)[self._tokens]\n",
    "    \n",
    "    # TO-DO: @properties\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._tokens[idx], self._targets[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "#### Neural net architecture for raw RNN - to be called once per epoch. Returns gradients and last hidden state from sequence to iterate through batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModelRNN:\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, clip_ratio, seq_len, batch_size, **params):\n",
    "        \n",
    "        \"\"\" Initialize necessary hyperparams and nn layers. Import updated params (or zero params).\n",
    "        Reset all grads \n",
    "        \"\"\"\n",
    "        \n",
    "        self._clip_ratio = clip_ratio\n",
    "        self._seq_len = seq_len\n",
    "        self._batch_size = batch_size\n",
    "        self._hidden_layer = np.zeros((self._seq_len,hidden_size,self._batch_size))\n",
    "        self._output_layer = np.zeros((self._seq_len,vocab_size,self._batch_size))\n",
    "        \n",
    "        self._Whx = Whx\n",
    "        self._Whh = Whh\n",
    "        self._Why = Why\n",
    "        self._bh = bh\n",
    "        self._by = by\n",
    "        \n",
    "        self._dWhx, self._dWhh, self._dWhy = np.zeros_like(Whx), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        self._dbh, self._dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        \n",
    "    def _forward_rnn(self, x, hidden_back, y=None):\n",
    "        \n",
    "        # Using hidden state from last RNN-cell of the previous sequence\n",
    "\n",
    "        self._hidden_layer[-1]= hidden_back\n",
    "        loss = 0\n",
    "        y = y_train\n",
    "        \n",
    "        # Batch Forward RNN propagation \n",
    "        \n",
    "        for t in range(self._seq_len):\n",
    "            self._hidden_layer[t] = np.tanh(np.dot(self._Whx, x[:,t].T)+\n",
    "                                            np.dot(self._Whh, self._hidden_layer[t-1])+ self._bh)\n",
    "            output = np.dot(self._Why, self._hidden_layer[t]) + self._by\n",
    "            self._output_layer[t] = np.exp(output) / np.sum(np.exp(output))\n",
    "            loss+= np.sum(-np.log(self._output_layer[t].T[range(self._batch_size), \n",
    "                                                            y[:,t]])) / self._batch_size\n",
    "       \n",
    "        return loss, self._hidden_layer, self._output_layer\n",
    "    \n",
    "    def _backward_rnn(self, x, y, hidden_layer, output_layer):\n",
    "        \n",
    "        \"\"\"Batch Backward RNN propagation. dhnext is\n",
    "        a derivative of previous(next in time) RNN cell hidden state. Init with zeros\n",
    "        Special thanks to Andrey Karpathy's solution:\n",
    "        https://gist.github.com/karpathy/d4dee566867f8291f086 \n",
    "        \"\"\"\n",
    "        dhnext = np.zeros_like(hidden_layer[0])\n",
    "        for t in reversed(range(self._seq_len)):\n",
    "            dy = np.copy(output_layer[t])\n",
    "            dy[y[:,t]] -= 1\n",
    "            self._dWhy += np.dot(dy, hidden_layer[t].T)\n",
    "            self._dby += np.sum(dy, axis=1).reshape(-1,1)\n",
    "            dh = np.dot(self._Why.T, dy) + dhnext\n",
    "            dhtan = (1 - hidden_layer[t] **2) * dh \n",
    "            self._dbh += np.sum(dhtan, axis=1).reshape(-1,1)\n",
    "            self._dWhx += np.dot(dhtan, x[:,t])\n",
    "            self._dWhh += np.dot(dhtan, hidden_layer[t-1].T)\n",
    "            dhnext = np.dot(self._Whh.T, dhtan)\n",
    "        \n",
    "        # Clipping gradients to omit explosions during learning. Utility method \n",
    "        \n",
    "        for dparam in [self._dWhx, self._dWhh, self._dWhy, self._dbh, self._dby]:\n",
    "            np.clip(dparam, -self._clip_ratio, self._clip_ratio, out=dparam)\n",
    "            \n",
    "        grads = {'dWhx': self._dWhx, 'dWhh': self._dWhh, 'dWhy': self._dWhy, \n",
    "                 'dbh': self._dbh, 'dby': self._dby}\n",
    "        \n",
    "        return grads, self._hidden_layer[self._seq_len-1]\n",
    "        \n",
    "    def batch_propagate(self, x_train, y_train, hidden_back, mode = 'train'):\n",
    "        loss, hidden_layer, output_layer = self._forward_rnn(x_train, hidden_back, \n",
    "                                                             y=y_train)\n",
    "        if mode == 'train':\n",
    "            grads, hidden_back = self._backward_rnn(x_train, y_train, hidden_layer, \n",
    "                                                output_layer)\n",
    "            return loss, grads, hidden_back\n",
    "        else:\n",
    "            return loss, hidden_layer[self._seq_len-1]\n",
    "        #elif model == 'eval':\n",
    "         #    loss, hidden_layer[self._seq_len-1]\n",
    "        \n",
    "        # Returning list of gradients as well as hidden state from last RNN-cell\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "#### Generate input/output batches from datasource for train/test modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataLoader(CharDatasource):\n",
    "    \n",
    "    def __init__(self, split_factor, batch_size, mode='train', **kwargs):\n",
    "        \n",
    "        \"\"\" CharDatasource is used as parent object.\n",
    "        train/test mode defines which part of dataset to use for batching\n",
    "        \"\"\"\n",
    "        \n",
    "        self._split_factor = split_factor\n",
    "        self._mode = mode\n",
    "        self._batch_size = batch_size\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \"\"\"Just to indicate - we are encountering chunks utility function for the second time.\n",
    "        In project this should go to utility - otherwise it's a bad practice\n",
    "        \"\"\"\n",
    "    \n",
    "        def chunks(l, n):\n",
    "            for i in range(0, len(l), n): yield l[i:i + n]\n",
    "        \n",
    "        \"\"\"Splitting dataset into train/test\n",
    "        \"\"\"\n",
    "        \n",
    "        train_set = (self._tokens[:int(self.__len__()*split_factor)], \n",
    "                     self._targets[:int(self.__len__()*split_factor)])\n",
    "        test_set = (self._tokens[int(self.__len__()*split_factor):], \n",
    "                    self._targets[int(self.__len__()*split_factor):])\n",
    "        \n",
    "        \"\"\"Eventhough we are having pretty small dataset, it's a \n",
    "        good practice to use generators instead of keeping\n",
    "        additional data arrays to free up memory\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._mode == 'train':\n",
    "            self._x_loader = chunks(train_set[0], self._batch_size)\n",
    "            self._y_loader = chunks(train_set[1], self._batch_size)\n",
    "        else:\n",
    "            self._x_loader = chunks(test_set[0], self._batch_size)\n",
    "            self._y_loader = chunks(test_set[1], self._batch_size)\n",
    "            \n",
    "        self._batches=[]\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                x, y = next(self._x_loader), next(self._y_loader)\n",
    "                if len(x)==self._batch_size:\n",
    "                    self._batches.append((x,y))\n",
    "\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def _getbatch(self, idx):\n",
    "        return self._batches[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "#### Trainer brings together everything we created before, to iterate via epochs and update parameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "\n",
    "def sample(h, seed_ix):\n",
    "    \"\"\" \n",
    "      sample a sequence of integers from the model \n",
    "      h is memory state, seed_ix is seed letter for first time step\n",
    "      \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    \n",
    "    while True:\n",
    "        h = np.tanh(np.dot(Whx, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heiyd<\n",
      "rldqubdpthppgvguaaygcbmaezkwvmyincofhaqikdkvhdtkberarnbeoimkpnblrclcvoidklckqveebrtubmcdakmtmp<\n",
      "zkbohmplkrttbhmukvubacl<\n",
      "ivaykfdekmqvrvnb<\n",
      "misbdvsrfm<\n",
      "iikltd<\n",
      "<\n",
      "cvyanuhcdwedibliyvgymmooybxvkkeubeolskedzbvranhaffpydpibronih<\n",
      "ecsvgjiptgvitrabqtnqollacffldgnydvbkkgiogpamtdyr<\n",
      "qhayhhvudaiadevxhhrwavk<\n",
      "efkiedutlbsmooru<\n",
      "mdirxpqidcv<\n",
      "ybphaibvqipeikkpbddkkkwhamdmri<\n",
      "elscsafarcksfs<\n",
      "vl<\n",
      "id<\n",
      "hrsdogsayvksigbbdkikwhhhyhqjocjbcopkb<\n",
      "vyoiwhrtdpqweukslwcsciddclc<\n",
      "mhkaxhkiqyzwfktvqbybotwmplhkymloffou<\n",
      "<\n",
      "zclphkmkmiylyk<\n",
      "oyvifemmkcnsyvelkalmknfmnuvaknhvuoiutcnbiovsbicowfmkviegiehkdosevkxlxcaeytgscg<\n",
      "rsibmo<\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 8\n",
    "vocab_size = 27\n",
    "seq_len = 15\n",
    "clip_ratio  = 1\n",
    "split_factor = 0.8\n",
    "path = 'test.txt'\n",
    "\n",
    "Whx = np.random.randn(hidden_size, vocab_size)*0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))\n",
    "params = {'Whx':Whx, 'Whh':Whh,'Why':Why,'bh':bh,'by':by}\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Whx), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "char_dataset = CharDatasource(path=path, seq_len=seq_len)\n",
    "\n",
    "data_train = RNNDataLoader(split_factor=split_factor, batch_size=batch_size, \n",
    "                           path=path,  seq_len=seq_len)\n",
    "data_test = RNNDataLoader(split_factor=split_factor, batch_size=batch_size, \n",
    "                          path=path,  seq_len=seq_len, mode='test')\n",
    "\n",
    "model = CharModelRNN(vocab_size=vocab_size, hidden_size=hidden_size, \n",
    "                     clip_ratio=clip_ratio, seq_len=seq_len, \n",
    "                     batch_size=batch_size, **params)\n",
    "train_epoch_loss = {}\n",
    "valid_epoch_loss = {}\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    \n",
    "    bulk_grads = []\n",
    "    epoch_grads = {'dWhx':0, 'dWhh':0, 'dWhy':0, 'dbh':0, 'dby':0}\n",
    "    hidden_back = np.zeros((hidden_size, batch_size))\n",
    "    train_losses = []\n",
    "    \n",
    "    hidden_back = np.zeros((hidden_size, batch_size))\n",
    "    hidden_sample = np.zeros((hidden_size,1))\n",
    "    \n",
    "    if n % 10 == 0:\n",
    "        sample_ix = sample(hidden_sample, 1)\n",
    "        txt = ''.join(char_dataset._idx2char[ix] for ix in sample_ix)\n",
    "        print(txt)\n",
    "    \n",
    "    for batch in data_train._batches:\n",
    "            x_train, y_train = batch[0], batch[1]\n",
    "            loss, grads, hidden_back = model.batch_propagate(x_train, \n",
    "                                                             y_train, hidden_back)\n",
    "            bulk_grads.append(grads)\n",
    "            train_losses.append(loss)\n",
    "  \n",
    "    train_epoch_loss.update({n:np.mean(train_losses)})\n",
    "\n",
    "    for i in range(len(bulk_grads)):\n",
    "        epoch_grads['dWhx'] += bulk_grads[i]['dWhx']\n",
    "        epoch_grads['dWhh'] += bulk_grads[i]['dWhh']\n",
    "        epoch_grads['dWhy'] += bulk_grads[i]['dWhy']\n",
    "        epoch_grads['dbh'] += bulk_grads[i]['dbh']\n",
    "        epoch_grads['dby'] += bulk_grads[i]['dby']\n",
    "    for k in epoch_grads.keys():\n",
    "        epoch_grads[k] = epoch_grads[k]/batch_size\n",
    "    \n",
    "    \"\"\"for p in params.keys():\n",
    "        params[p] += - learning_rate * epoch_grads[str('d'+p)]\n",
    "    \"\"\"    \n",
    "    for param, dparam, mem in zip([params['Whx'], params['Whh'], params['Why'], \n",
    "                                   params['bh'], params['by']], \n",
    "                                [epoch_grads['dWhx'],  epoch_grads['dWhh'], epoch_grads['dWhy'], \n",
    "                                 epoch_grads['dbh'], epoch_grads['dby']], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    \n",
    "    test_losses = []\n",
    "    for batch in data_test._batches:\n",
    "        x_train, y_train = batch[0], batch[1]\n",
    "        loss, hidden_back = model.batch_propagate(x_train, \n",
    "                                                             y_train, hidden_back, mode='eval')\n",
    "        test_losses.append(loss)\n",
    "\n",
    "    valid_epoch_loss.update({n:np.mean(test_losses)})\n",
    "    \n",
    "x = train_epoch_loss\n",
    "t = valid_epoch_loss\n",
    "plt.plot(t.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dataloader -> Trainer -> Inference\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experiment\n",
    "\n",
    "\"\"\" Dataloader -> Trainer -> Inference\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
